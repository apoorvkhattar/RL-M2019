{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import sys\n",
    "\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 5\n",
    "A, A_d = (0,1), (4,1)\n",
    "B, B_d = (0,3), (2,3)\n",
    "\n",
    "gamma = 0.9 # Given\n",
    "pi_action = [0.25, 0.25, 0.25, 0.25] # Policy function\n",
    "actions = [(-1,0), (1,0), (0,1), (0, -1)] # Set of actions i.e. left, right, up, down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = GRID_SIZE * GRID_SIZE # Every cell is a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside(GRID_SIZE, new_location):\n",
    "    H,W = GRID_SIZE, GRID_SIZE\n",
    "    if(new_location[0]<0):\n",
    "        return False\n",
    "    if(new_location[0]>=H):\n",
    "        return False\n",
    "    if(new_location[1]<0):\n",
    "        return False\n",
    "    if(new_location[1]>=W):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_equations(actions, pi_action):\n",
    "    mat_A = np.zeros((num_states, num_states)) # matrix (A)\n",
    "    mat_B = np.zeros((num_states,1)) # vector (b), to solve Ax=b\n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            pos = i * GRID_SIZE + j\n",
    "            if (i,j) == A: # Special case if agent is at (0,1), new state = (4,1), reward = 10\n",
    "                new_location = A_d[0] * GRID_SIZE + A_d[1]\n",
    "                mat_A[pos, new_location] -= gamma \n",
    "                mat_B[pos] += 10\n",
    "            elif (i,j) == B: # Special case if agent is at (0,3), new state = (2,3), reward = 5\n",
    "                new_location = B_d[0] * GRID_SIZE + B_d[1]\n",
    "                mat_A[pos, new_location] -= gamma\n",
    "                mat_B[pos] += 5\n",
    "            else:\n",
    "                for k in range(len(actions)):\n",
    "                    # For every action, agent goes to a state\n",
    "                    # i.e. if action = right at state (0,0) then agent goes to (0,1) and has reward 0 with prob = 1\n",
    "                    new_location = i + actions[k][0], j + actions[k][1]\n",
    "                    if inside(GRID_SIZE, new_location):\n",
    "                        pos_2 = new_location[0] * GRID_SIZE + new_location[1]\n",
    "                        mat_A[pos,pos_2] -= pi_action[k] * gamma\n",
    "                        mat_B[pos] += 0\n",
    "                    else:\n",
    "                        mat_A[pos,pos] -= pi_action[k] * gamma\n",
    "                        mat_B[pos] += -1 * pi_action[k]\n",
    "            mat_A[pos,pos] += 1\n",
    "    return mat_A, mat_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = create_equations(actions, pi_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.3,  8.8,  4.4,  5.3,  1.5],\n",
       "       [ 1.5,  3. ,  2.3,  1.9,  0.5],\n",
       "       [ 0.1,  0.7,  0.7,  0.4, -0.4],\n",
       "       [-1. , -0.4, -0.4, -0.6, -1.2],\n",
       "       [-1.9, -1.3, -1.2, -1.4, -2. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function = np.linalg.solve(A, b)\n",
    "value_function = np.reshape(value_function, (GRID_SIZE,GRID_SIZE))\n",
    "np.set_printoptions(precision=1)\n",
    "value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 5\n",
    "A, A_d = (0,1), (4,1)\n",
    "B, B_d = (0,3), (2,3)\n",
    "\n",
    "gamma = 0.9 # Given\n",
    "pi_action = [1, 1, 1, 1] # Policy function\n",
    "actions = [(-1,0), (1,0), (0,1), (0, -1)] # Set of actions i.e. left, right, up, down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = GRID_SIZE * GRID_SIZE # Every cell is a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_equations(actions, pi_action):\n",
    "    mat_A = np.zeros((num_states * len(actions), num_states)) # matrix (A)\n",
    "    mat_B = np.zeros((num_states * len(actions),1)) # vector (b), to solve Ax=b\n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            pos = i * GRID_SIZE + j\n",
    "            if (i,j) == A: # Special case if agent is at (0,1), new state = (4,1), reward = 10\n",
    "                new_location = A_d[0] * GRID_SIZE + A_d[1]\n",
    "                for k in range(len(actions)):\n",
    "                    mat_A[pos + k, new_location] -= gamma \n",
    "                    mat_B[pos + k] += 10\n",
    "            elif (i,j) == B: # Special case if agent is at (0,3), new state = (2,3), reward = 5\n",
    "                new_location = B_d[0] * GRID_SIZE + B_d[1]\n",
    "                for k in range(len(actions)):\n",
    "                    mat_A[pos + k, new_location] -= gamma\n",
    "                    mat_B[pos + k] += 5\n",
    "            else:\n",
    "                for k in range(len(actions)):\n",
    "                    new_location = i + actions[k][0], j + actions[k][1]\n",
    "                    if inside(GRID_SIZE, new_location):\n",
    "                        pos_2 = new_location[0] * GRID_SIZE + new_location[1]\n",
    "                        mat_A[pos + k, pos_2] -= pi_action[k] * gamma\n",
    "                        mat_B[pos + k] += 0\n",
    "                    else:\n",
    "                        mat_A[pos + k, pos] -= pi_action[k] * gamma\n",
    "                        mat_B[pos + k] += -1 * pi_action[k]\n",
    "            for k in range(len(actions)):\n",
    "                mat_A[pos + k, pos] += 1\n",
    "    return mat_A * -1, mat_B * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mA, mb = create_equations(actions, pi_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     con: array([], dtype=float64)\n",
       "     fun: -501.1865808837265\n",
       " message: 'Optimization failed. The problem appears to be unbounded.'\n",
       "     nit: 41\n",
       "   slack: array([ 3.27699169e+00, -1.24344979e-14, -1.42108547e-14,  8.15628650e+00,\n",
       "        1.19015908e-13, -1.03739239e-12, -6.07514039e-13,  6.27053964e-13,\n",
       "        5.32907052e-13,  1.52766688e-13,  3.11653462e+01, -7.81597009e-14,\n",
       "       -1.42108547e-14,  7.10542736e-15,  9.23705556e-14,  1.24344979e-13,\n",
       "        3.68722465e+01, -9.23705556e-14, -3.19744231e-14,  5.28805001e+01,\n",
       "        1.59872116e-14, -1.68753900e-14, -4.61852778e-14,  9.61762502e+00,\n",
       "       -1.42108547e-14, -2.13162821e-14, -7.32747196e-15, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00])\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([22.76991695, 35.19854115, 18.02905475, 32.81064425,  0.        ,\n",
       "       32.08420173, 21.15857741, 26.27809446, 22.59199399,  2.59589407,\n",
       "       29.95416953, 16.35824981, 23.39691244, 26.01775976,  0.        ,\n",
       "       30.52333117, 15.11080157, 31.5314702 , 14.5132011 , 42.5075844 ,\n",
       "        0.        , 21.21408504, 14.9038878 , 11.38853121, 10.24967809])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.ones((num_states,1)) * -1\n",
    "value_function = linprog(C, A_ub=mA, b_ub=mb)\n",
    "value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 4\n",
    "terminal_states = [(0,0), (3,3)]\n",
    "num_states = GRID_SIZE * GRID_SIZE\n",
    "\n",
    "gamma = 1.0 # Given\n",
    "actions = [(-1,0), (1,0), (0,1), (0, -1)] # Set of actions i.e. left, right, up, down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(expected_return):\n",
    "    indices = []\n",
    "    max_val = np.max(expected_return)\n",
    "    for i in range(expected_return.shape[0]):\n",
    "        if expected_return[i] == max_val:\n",
    "            indices.append(i)\n",
    "    indices = np.array(indices)\n",
    "    indices = np.reshape(indices, (indices.shape[0],1))\n",
    "    indices = indices.astype(np.uint8)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using policy interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update value function for policy\n",
      " [[  0.         -12.95986164 -18.45870225 -20.27519666]\n",
      " [-12.95986164 -16.64220783 -18.46901249 -18.45870225]\n",
      " [-18.45870225 -18.46901249 -16.64220783 -12.95986164]\n",
      " [-20.27519666 -18.45870225 -12.95986164   0.        ]]\n",
      "Updated policy using new value function\n",
      " [[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.5 0.  0.5]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.5 0.  0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0. ]\n",
      " [0.  0.5 0.5 0. ]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.  0.5 0. ]\n",
      " [0.  0.  1.  0. ]\n",
      " [0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0. ]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal policy***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Updated policy using new value function\n",
      " [[0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.5  0.   0.5 ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.5  0.   0.5  0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   0.  ]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal policy***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Updated policy using new value function\n",
      " [[0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.5  0.   0.5 ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.5  0.   0.5  0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   0.  ]]\n",
      "\n",
      "\n",
      "***Found optimal policy***\n",
      "\n",
      "\n",
      "The optimal policy is\n",
      "\n",
      "(0, 0): [0. 0. 0. 0.]\n",
      "(0, 1): [0. 0. 0. 1.]\n",
      "(0, 2): [0. 0. 0. 1.]\n",
      "(0, 3): [0.  0.5 0.  0.5]\n",
      "(1, 0): [1. 0. 0. 0.]\n",
      "(1, 1): [0.5 0.  0.  0.5]\n",
      "(1, 2): [0.25 0.25 0.25 0.25]\n",
      "(1, 3): [0. 1. 0. 0.]\n",
      "(2, 0): [1. 0. 0. 0.]\n",
      "(2, 1): [0.25 0.25 0.25 0.25]\n",
      "(2, 2): [0.  0.5 0.5 0. ]\n",
      "(2, 3): [0. 1. 0. 0.]\n",
      "(3, 0): [0.5 0.  0.5 0. ]\n",
      "(3, 1): [0. 0. 1. 0.]\n",
      "(3, 2): [0. 0. 1. 0.]\n",
      "(3, 3): [0. 0. 0. 0.]\n",
      "\n",
      "Order of actions is down, up, right, left\n"
     ]
    }
   ],
   "source": [
    "V_s = np.zeros((num_states,1)) # Initialization of the value function\n",
    "pi_action = np.ones((num_states,4)) * 0.25\n",
    "tolerance = 0.1\n",
    "\n",
    "policy_stable = False\n",
    "while not policy_stable:\n",
    "    \n",
    "    # Policy evaluation\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        v_s = np.copy(V_s)\n",
    "        V_s = np.zeros(V_s.shape)\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                if (i,j) in terminal_states:\n",
    "                    continue\n",
    "                else:\n",
    "                    s = i * GRID_SIZE + j\n",
    "                    for k in range(len(actions)):\n",
    "                        new_location = i + actions[k][0], j + actions[k][1]\n",
    "                        if inside(GRID_SIZE, new_location):\n",
    "                            new_s = new_location[0] * GRID_SIZE + new_location[1]\n",
    "                            V_s[s] += (-1 + gamma * v_s[new_s]) * pi_action[s][k]\n",
    "                        else:\n",
    "                            V_s[s] += (-1 + gamma * v_s[s]) * pi_action[s][k]\n",
    "                    delta = max(delta, np.absolute(V_s[s] - v_s[s]))\n",
    "        if delta < tolerance:\n",
    "            break\n",
    "    \n",
    "    print('Update value function for policy\\n', V_s.reshape((GRID_SIZE,GRID_SIZE)))\n",
    "            \n",
    "    # Policy improvement\n",
    "    old_policy = np.copy(pi_action)\n",
    "    pi_action = np.zeros(pi_action.shape)\n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            if (i,j) in terminal_states:\n",
    "                continue\n",
    "            else:\n",
    "                s = i * GRID_SIZE + j\n",
    "                expected_return = np.zeros((len(actions),1))\n",
    "                for k in range(len(actions)):\n",
    "                    new_location = i + actions[k][0], j + actions[k][1]\n",
    "                    if inside(GRID_SIZE, new_location):\n",
    "                        new_s = new_location[0] * GRID_SIZE + new_location[1]\n",
    "                        expected_return[k] = (-1 + gamma * V_s[new_s])\n",
    "                    else:\n",
    "                        expected_return[k] = (-1 + gamma * V_s[s])\n",
    "                best_actions = argmax(expected_return)\n",
    "                pi_action[s][best_actions] = 1\n",
    "                pi_action[s,:] /= np.sum(pi_action[s,:])\n",
    "    print('Updated policy using new value function\\n', pi_action)\n",
    "    if np.all(pi_action == old_policy):\n",
    "        print('\\n\\n***Found optimal policy***\\n\\n')\n",
    "        break\n",
    "    print('\\n\\n ***Iteration Complete, still searching for optimal policy***\\n\\n')\n",
    "\n",
    "print('The optimal policy is\\n')\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        print('{}: {}'.format((i,j), pi_action[i*GRID_SIZE+j]))\n",
    "        \n",
    "print(\"\\nOrder of actions is down, up, right, left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update value function for policy\n",
      " [[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal value function***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal value function***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal value function***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "***Found oprimal value function***\n",
      "\n",
      "\n",
      "The optimal policy is\n",
      "\n",
      "(0, 0): [0. 0. 0. 0.]\n",
      "(0, 1): [0. 0. 0. 1.]\n",
      "(0, 2): [0. 0. 0. 1.]\n",
      "(0, 3): [0.  0.5 0.  0.5]\n",
      "(1, 0): [1. 0. 0. 0.]\n",
      "(1, 1): [0.5 0.  0.  0.5]\n",
      "(1, 2): [0.25 0.25 0.25 0.25]\n",
      "(1, 3): [0. 1. 0. 0.]\n",
      "(2, 0): [1. 0. 0. 0.]\n",
      "(2, 1): [0.25 0.25 0.25 0.25]\n",
      "(2, 2): [0.  0.5 0.5 0. ]\n",
      "(2, 3): [0. 1. 0. 0.]\n",
      "(3, 0): [0.5 0.  0.5 0. ]\n",
      "(3, 1): [0. 0. 1. 0.]\n",
      "(3, 2): [0. 0. 1. 0.]\n",
      "(3, 3): [0. 0. 0. 0.]\n",
      "\n",
      "Order of actions is down, up, right, left\n"
     ]
    }
   ],
   "source": [
    "V_s = np.zeros((num_states,1)) # Initialization of the value function\n",
    "pi_action = np.ones((num_states,4)) * 0.25\n",
    "tolerance = 0.1\n",
    "\n",
    "while True:\n",
    "    delta = 0.0\n",
    "    v_s = np.copy(V_s)\n",
    "    V_s = np.zeros(V_s.shape)\n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            s = i * GRID_SIZE + j\n",
    "            if (i,j) in terminal_states:\n",
    "                continue\n",
    "            else:\n",
    "                expected_return = np.zeros((len(actions),1))\n",
    "                for k in range(len(actions)):\n",
    "                    new_location = i + actions[k][0], j + actions[k][1]\n",
    "                    if inside(GRID_SIZE, new_location):\n",
    "                        s_new = new_location[0] * GRID_SIZE + new_location[1]\n",
    "                        expected_return[k] = (-1 + gamma * v_s[s_new])\n",
    "                    else:\n",
    "                        expected_return[k] = (-1 + gamma * v_s[s])\n",
    "                V_s[s] = np.max(expected_return)\n",
    "                \n",
    "                delta = max(delta, np.absolute(V_s[s] - v_s[s]))\n",
    "    \n",
    "    print('Update value function for policy\\n', V_s.reshape((GRID_SIZE,GRID_SIZE)))\n",
    "    \n",
    "    if delta < tolerance:\n",
    "        print('\\n\\n***Found oprimal value function***\\n\\n')\n",
    "        break\n",
    "    print('\\n\\n ***Iteration Complete, still searching for optimal value function***\\n\\n')\n",
    "    \n",
    "pi_action = np.zeros(pi_action.shape)\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        if (i,j) in terminal_states:\n",
    "            continue\n",
    "        else:\n",
    "            s = i * GRID_SIZE + j\n",
    "            expected_return = np.zeros((len(actions),1))\n",
    "            for k in range(len(actions)):\n",
    "                new_location = i + actions[k][0], j + actions[k][1]\n",
    "                if inside(GRID_SIZE, new_location):\n",
    "                    new_s = new_location[0] * GRID_SIZE + new_location[1]\n",
    "                    expected_return[k] = (-1 + gamma * V_s[new_s])\n",
    "                else:\n",
    "                    expected_return[k] = (-1 + gamma * V_s[s])\n",
    "            best_actions = argmax(expected_return)\n",
    "            pi_action[s][best_actions] = 1\n",
    "            pi_action[s,:] /= np.sum(pi_action[s,:])\n",
    "            \n",
    "print('The optimal policy is\\n')\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        print('{}: {}'.format((i,j), pi_action[i*GRID_SIZE+j]))\n",
    "        \n",
    "print(\"\\nOrder of actions is down, up, right, left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Jack's Car Rental Problem with Max. Number of Cars = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cars = 10\n",
    "num_states = (num_cars + 1) * (num_cars + 1)\n",
    "\n",
    "num_rentals_1 = 3\n",
    "num_returns_1 = 3\n",
    "\n",
    "num_rentals_2 = 4\n",
    "num_returns_2 = 2\n",
    "\n",
    "r_rent = 10\n",
    "r_move = -2\n",
    "\n",
    "max_move = 5\n",
    "\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson(x, lamb):\n",
    "    p = np.exp(-lamb) * np.power(lamb, x) / np.math.factorial(x)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_return(state, action, value_function):\n",
    "    e_return = 0.0\n",
    "    e_return += r_move * np.absolute(action)\n",
    "    new_state = min(state[0] - action, num_cars), min(state[1] + action, num_cars)\n",
    "    new_state = int(new_state[0]), int(new_state[1])\n",
    "    for i in range(0, new_state[0] + 1):\n",
    "        for j in range(0, new_state[1] + 1):\n",
    "            p_rent = poisson(i, num_rentals_1) * poisson(j, num_rentals_2)\n",
    "            rent_reward = (i + j) * r_rent\n",
    "            for k in range(0, 11):\n",
    "                for l in range(0, 11):\n",
    "                    p_return = poisson(k, num_returns_1) * poisson(l, num_returns_2)\n",
    "                    final_state = new_state[0] - i + k, new_state[1] - j + l\n",
    "                    final_state = min(final_state[0], num_cars), min(final_state[1], num_cars)\n",
    "                    n_s = final_state[0] * (num_cars + 1) + final_state[1]\n",
    "                    e_return += p_return * p_rent * (rent_reward + gamma * value_function[int(n_s)])\n",
    "    \n",
    "    return e_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update value function for policy\n",
      " [[0.00000000e+00 3.64643126e-02 1.82325596e-01 4.74083652e-01\n",
      "  8.63191911e-01 1.25245145e+00 1.56404443e+00 1.77197599e+00\n",
      "  1.89099401e+00 1.95067999e+00 1.97734863e+00]\n",
      " [2.73843741e-02 2.82814957e-01 1.08579509e+00 2.54781630e+00\n",
      "  4.40497724e+00 6.21135405e+00 7.63448754e+00 8.57839708e+00\n",
      "  9.12122426e+00 9.39882705e+00 9.52825420e+00]\n",
      " [1.10516959e-01 8.63466228e-01 2.99486518e+00 6.68766832e+00\n",
      "  1.12629753e+01 1.56674100e+01 1.91436460e+01 2.14870081e+01\n",
      "  2.28850654e+01 2.36494510e+01 2.40465709e+01]\n",
      " [2.39953347e-01 1.67960152e+00 5.53392157e+00 1.20324552e+01\n",
      "  1.99977826e+01 2.76766833e+01 3.38235499e+01 3.80975988e+01\n",
      "  4.07895559e+01 4.23890469e+01 4.33185029e+01]\n",
      " [3.78185205e-01 2.50493749e+00 8.02201136e+00 1.71860243e+01\n",
      "  2.83779250e+01 3.92353369e+01 4.80810633e+01 5.44342276e+01\n",
      "  5.86431806e+01 6.13216229e+01 6.30075739e+01]\n",
      " [4.91505527e-01 3.16042420e+00 9.96289938e+00 2.11752453e+01\n",
      "  3.48677926e+01 4.82504632e+01 5.93346003e+01 6.75190445e+01\n",
      "  7.31624698e+01 7.69371182e+01 7.94421776e+01]\n",
      " [5.68032347e-01 3.59492202e+00 1.12382474e+01 2.37949790e+01\n",
      "  3.91570887e+01 5.42819970e+01 6.69873992e+01 7.65806181e+01\n",
      "  8.34003684e+01 8.81288685e+01 9.13830769e+01]\n",
      " [6.13376586e-01 3.84987655e+00 1.19856944e+01 2.53403271e+01\n",
      "  4.17210263e+01 5.79549976e+01 7.17506705e+01 8.23506411e+01\n",
      "  9.00620005e+01 9.55516931e+01 9.94295300e+01]\n",
      " [6.38378302e-01 3.99006305e+00 1.23988683e+01 2.62056230e+01\n",
      "  4.31846057e+01 6.01022153e+01 7.46085271e+01 8.59028371e+01\n",
      "  9.42615674e+01 1.00328653e+02 1.04696825e+02]\n",
      " [6.51848608e-01 4.06578589e+00 1.26242758e+01 2.66856474e+01\n",
      "  4.40146848e+01 6.13513785e+01 7.63155843e+01 8.80791679e+01\n",
      "  9.68944171e+01 1.03383751e+02 1.08121572e+02]\n",
      " [6.59167371e-01 4.10960949e+00 1.28301415e+01 2.75679294e+01\n",
      "  4.66606724e+01 6.70896057e+01 8.62875720e+01 1.03045655e+02\n",
      "  1.17203792e+02 1.29018117e+02 1.38785506e+02]]\n",
      "Updated policy using new value function\n",
      " [[ 0.  0.  0.  0. -1. -2. -2. -3. -3. -4. -4.]\n",
      " [ 0.  0.  0.  0. -1. -1. -2. -2. -3. -3. -4.]\n",
      " [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -2.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1. -1. -2.]\n",
      " [ 2.  2.  2.  1.  1.  1.  0.  0.  0. -1. -1.]\n",
      " [ 3.  3.  2.  2.  2.  1.  1.  0.  0.  0. -1.]\n",
      " [ 4.  3.  3.  3.  2.  2.  1.  1.  0.  0.  0.]\n",
      " [ 4.  4.  4.  3.  3.  2.  2.  1.  1.  0.  0.]\n",
      " [ 5.  5.  4.  4.  3.  3.  2.  2.  1.  1.  0.]\n",
      " [ 5.  5.  5.  4.  4.  3.  3.  2.  2.  1.  0.]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal policy***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[0.00000000e+00 3.64643126e-02 1.82325596e-01 4.74083652e-01\n",
      "  5.43837040e-01 2.61932550e+00 7.09476568e+00 1.32064230e+01\n",
      "  2.02560204e+01 2.77555977e+01 3.48874498e+01]\n",
      " [2.74025547e-02 2.82948444e-01 1.08635747e+00 2.54961387e+00\n",
      "  4.63718286e+00 9.14081691e+00 1.52809210e+01 2.24048520e+01\n",
      "  2.99035175e+01 3.71349937e+01 4.26971180e+01]\n",
      " [1.10740498e-01 8.65026091e-01 3.00103968e+00 6.70611350e+00\n",
      "  1.13089195e+01 1.75544046e+01 2.49134452e+01 3.24692867e+01\n",
      "  4.00170198e+01 4.53988826e+01 5.10511810e+01]\n",
      " [2.40502196e-01 1.68343057e+00 5.54909951e+00 1.20779579e+01\n",
      "  2.01116987e+01 2.79242430e+01 3.57090824e+01 4.38956638e+01\n",
      "  4.91427682e+01 5.53968108e+01 5.93462671e+01]\n",
      " [3.78838787e-01 3.81643025e+00 1.06420075e+01 1.91139632e+01\n",
      "  2.86089583e+01 3.96330622e+01 4.87516662e+01 5.41984625e+01\n",
      "  6.14209455e+01 6.63066374e+01 6.68552968e+01]\n",
      " [2.25494591e+00 9.55771114e+00 1.86432250e+01 2.83901096e+01\n",
      "  4.03352320e+01 5.04568418e+01 6.03868937e+01 6.88183090e+01\n",
      "  7.48278838e+01 7.53895084e+01 7.91152409e+01]\n",
      " [8.67455798e+00 1.84680313e+01 2.87711987e+01 4.17098884e+01\n",
      "  5.28344729e+01 6.35215086e+01 7.31266375e+01 7.87614085e+01\n",
      "  8.55152308e+01 9.02467468e+01 8.58482235e+01]\n",
      " [1.81720314e+01 2.94070249e+01 4.32857090e+01 5.52676840e+01\n",
      "  6.71485561e+01 7.77648770e+01 8.45582914e+01 9.23513777e+01\n",
      "  9.36193220e+01 9.84602292e+01 1.01750873e+02]\n",
      " [2.97359384e+01 4.43918527e+01 5.70175614e+01 7.06091246e+01\n",
      "  8.19985653e+01 9.04514504e+01 9.90760780e+01 1.01950909e+02\n",
      "  1.07604753e+02 1.05297882e+02 1.08460501e+02]\n",
      " [4.45703777e+01 5.76215671e+01 7.30926332e+01 8.49984971e+01\n",
      "  9.56567418e+01 1.04847918e+02 1.09836835e+02 1.16044780e+02\n",
      "  1.15749129e+02 1.19416214e+02 1.14358226e+02]\n",
      " [5.83442124e+01 7.41388194e+01 8.63770462e+01 9.95804237e+01\n",
      "  1.09223391e+02 1.18096325e+02 1.24884601e+02 1.31418143e+02\n",
      "  1.35930446e+02 1.43914799e+02 1.55875863e+02]]\n",
      "Updated policy using new value function\n",
      " [[ 0.  0.  0.  0. -1. -2. -2. -3. -3. -4. -4.]\n",
      " [ 0.  0.  0.  0. -1. -1. -2. -2. -3. -3. -4.]\n",
      " [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -3.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1. -2. -3.]\n",
      " [ 2.  2.  2.  1.  1.  0.  0.  0. -1. -2. -2.]\n",
      " [ 3.  3.  2.  2.  1.  1.  0.  0. -1. -1. -2.]\n",
      " [ 4.  3.  3.  2.  2.  1.  1.  0.  0. -1. -1.]\n",
      " [ 4.  4.  3.  3.  2.  2.  1.  0.  0.  0. -1.]\n",
      " [ 5.  4.  4.  3.  3.  2.  1.  1.  0.  0.  0.]\n",
      " [ 5.  5.  4.  4.  3.  2.  2.  1.  0.  0.  0.]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal policy***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[0.00000000e+00 3.64643126e-02 1.82325596e-01 4.74083652e-01\n",
      "  5.43837040e-01 2.61932550e+00 7.09476568e+00 1.32064230e+01\n",
      "  2.02560204e+01 2.77555977e+01 3.48874498e+01]\n",
      " [2.74025547e-02 2.82948444e-01 1.08635747e+00 2.54961387e+00\n",
      "  4.63718286e+00 9.14081691e+00 1.52809210e+01 2.24048520e+01\n",
      "  2.99035175e+01 3.71349937e+01 4.26971180e+01]\n",
      " [1.10740498e-01 8.65026091e-01 3.00103968e+00 6.70611350e+00\n",
      "  1.13089195e+01 1.75544046e+01 2.49134452e+01 3.24692867e+01\n",
      "  4.00170198e+01 4.53988826e+01 5.10511810e+01]\n",
      " [2.40502196e-01 1.68343057e+00 5.54909951e+00 1.20779579e+01\n",
      "  2.01116987e+01 2.79242430e+01 3.57090824e+01 4.38956638e+01\n",
      "  4.91427682e+01 5.53968108e+01 5.68814129e+01]\n",
      " [3.78838587e-01 3.81641849e+00 1.06419508e+01 1.91137272e+01\n",
      "  2.86087110e+01 3.96321645e+01 4.87487697e+01 5.41960532e+01\n",
      "  6.14142098e+01 6.23111037e+01 6.03582419e+01]\n",
      " [2.25482364e+00 9.55718793e+00 1.86412663e+01 2.83877808e+01\n",
      "  4.03275681e+01 4.90893847e+01 6.03346631e+01 6.87308796e+01\n",
      "  6.96872552e+01 6.68214630e+01 7.16017894e+01]\n",
      " [8.66710558e+00 1.84500105e+01 2.87464368e+01 4.16557985e+01\n",
      "  5.08991601e+01 6.33573567e+01 6.89873286e+01 7.84118324e+01\n",
      "  7.55030436e+01 8.12571472e+01 7.49639818e+01]\n",
      " [1.81211578e+01 2.93292357e+01 4.31355061e+01 5.32421609e+01\n",
      "  6.67837848e+01 7.32779830e+01 8.38431721e+01 8.57398613e+01\n",
      "  9.25480039e+01 8.60880959e+01 8.96661510e+01]\n",
      " [2.95996239e+01 4.41487620e+01 5.56246397e+01 7.00519951e+01\n",
      "  7.78693807e+01 8.94029679e+01 9.26308400e+01 9.19485784e+01\n",
      "  9.87718439e+01 1.03105714e+02 9.38207371e+01]\n",
      " [4.42476447e+01 5.72810568e+01 7.23310232e+01 8.19981793e+01\n",
      "  9.42069830e+01 9.92192194e+01 1.00095996e+02 1.07782354e+02\n",
      "  1.04431376e+02 1.08517070e+02 1.10909093e+02]\n",
      " [5.77596065e+01 7.32233881e+01 8.50233520e+01 9.77792561e+01\n",
      "  1.06093239e+02 1.12345622e+02 1.21522879e+02 1.29239199e+02\n",
      "  1.40440763e+02 1.47793953e+02 1.53246973e+02]]\n",
      "Updated policy using new value function\n",
      " [[ 0.  0.  0.  0. -1. -2. -2. -3. -3. -4. -4.]\n",
      " [ 0.  0.  0.  0. -1. -1. -2. -2. -3. -3. -4.]\n",
      " [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -3.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1. -2. -3.]\n",
      " [ 2.  2.  2.  1.  1.  0.  0.  0. -1. -2. -2.]\n",
      " [ 3.  3.  2.  2.  1.  1.  0.  0. -1. -1. -2.]\n",
      " [ 4.  3.  3.  2.  2.  1.  1.  0.  0. -1. -2.]\n",
      " [ 4.  4.  3.  3.  2.  2.  1.  0.  0. -1. -1.]\n",
      " [ 5.  4.  4.  3.  3.  2.  1.  0.  0.  0. -1.]\n",
      " [ 5.  5.  4.  4.  3.  2.  1.  1.  0.  0.  0.]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal policy***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[0.00000000e+00 3.64643126e-02 1.82325596e-01 4.74083652e-01\n",
      "  5.43837040e-01 2.61932550e+00 7.09476568e+00 1.32064230e+01\n",
      "  2.02560204e+01 2.77555977e+01 3.48874498e+01]\n",
      " [2.74025547e-02 2.82948444e-01 1.08635747e+00 2.54961387e+00\n",
      "  4.63718286e+00 9.14081691e+00 1.52809210e+01 2.24048520e+01\n",
      "  2.99035175e+01 3.71349937e+01 4.26971180e+01]\n",
      " [1.10740498e-01 8.65026091e-01 3.00103968e+00 6.70611350e+00\n",
      "  1.13089195e+01 1.75544046e+01 2.49134452e+01 3.24692867e+01\n",
      "  4.00170198e+01 4.53988826e+01 5.10511810e+01]\n",
      " [2.40502196e-01 1.68343057e+00 5.54909951e+00 1.20779579e+01\n",
      "  2.01116987e+01 2.79242430e+01 3.57090824e+01 4.38956638e+01\n",
      "  4.91427682e+01 5.53968108e+01 5.68814129e+01]\n",
      " [3.78838587e-01 3.81641849e+00 1.06419508e+01 1.91137272e+01\n",
      "  2.86087110e+01 3.96321645e+01 4.87487697e+01 5.41960532e+01\n",
      "  6.14142098e+01 6.23111037e+01 6.03582419e+01]\n",
      " [2.25482364e+00 9.55718793e+00 1.86412663e+01 2.83877808e+01\n",
      "  4.03275681e+01 4.90893847e+01 6.03346631e+01 6.87308796e+01\n",
      "  6.96872552e+01 6.68214630e+01 7.16017894e+01]\n",
      " [8.66710558e+00 1.84500105e+01 2.87464368e+01 4.16557985e+01\n",
      "  5.08991601e+01 6.33573567e+01 6.89873286e+01 7.84118324e+01\n",
      "  7.55030436e+01 8.12571472e+01 7.49639818e+01]\n",
      " [1.81211578e+01 2.93292357e+01 4.31355061e+01 5.32421609e+01\n",
      "  6.67837848e+01 7.32779830e+01 8.38431721e+01 8.57398613e+01\n",
      "  9.25480039e+01 8.60880959e+01 7.77902957e+01]\n",
      " [2.95989565e+01 4.41463146e+01 5.56207961e+01 7.00395720e+01\n",
      "  7.78528576e+01 8.93564204e+01 9.25786325e+01 9.18994134e+01\n",
      "  9.86544263e+01 9.01259331e+01 9.34149417e+01]\n",
      " [4.42411426e+01 5.72698293e+01 7.23004954e+01 8.19534956e+01\n",
      "  9.40994053e+01 9.90862632e+01 9.99560174e+01 9.73984653e+01\n",
      "  1.03944043e+02 1.07826636e+02 9.69902416e+01]\n",
      " [5.77283319e+01 7.31539764e+01 8.49081043e+01 9.75507838e+01\n",
      "  1.05770692e+02 1.11946715e+02 1.19220501e+02 1.28402568e+02\n",
      "  1.39563218e+02 1.46589967e+02 1.51651842e+02]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated policy using new value function\n",
      " [[ 0.  0.  0.  0. -1. -2. -2. -3. -3. -4. -4.]\n",
      " [ 0.  0.  0.  0. -1. -1. -2. -2. -3. -3. -4.]\n",
      " [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -3.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1. -2. -3.]\n",
      " [ 2.  2.  2.  1.  1.  0.  0.  0. -1. -2. -2.]\n",
      " [ 3.  3.  2.  2.  1.  1.  0.  0. -1. -1. -2.]\n",
      " [ 4.  3.  3.  2.  2.  1.  1.  0.  0. -1. -2.]\n",
      " [ 4.  4.  3.  3.  2.  2.  1.  0.  0. -1. -1.]\n",
      " [ 5.  4.  4.  3.  3.  2.  1.  0.  0.  0. -1.]\n",
      " [ 5.  5.  4.  4.  3.  2.  1.  1.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "***Found optimal policy***\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V_s = np.zeros((num_states,1)) # Value functions for all states\n",
    "pi_action = np.zeros((num_cars + 1, num_cars + 1)) # Initial policy to not move any cars\n",
    "tolerance = 1.0e-1\n",
    "\n",
    "policy_stable = False\n",
    "while not policy_stable:\n",
    "    # Policy evaluation\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        v_s = np.copy(V_s)\n",
    "        V_s = np.zeros(V_s.shape)\n",
    "        for i in range(num_cars + 1):\n",
    "            for j in range(num_cars + 1):\n",
    "                s = i * (num_cars + 1) + j\n",
    "                V_s[s] = expected_return((i,j), pi_action[i,j], V_s)\n",
    "                delta = max(delta, V_s[s] - v_s[s])\n",
    "        if delta < tolerance:\n",
    "            break\n",
    "            \n",
    "    print('Update value function for policy\\n', V_s.reshape((num_cars + 1,num_cars + 1)))\n",
    "    \n",
    "    # Policy improvement\n",
    "    old_policy = np.copy(pi_action)\n",
    "    pi_action = np.zeros(pi_action.shape)\n",
    "    for i in range(num_cars + 1):\n",
    "        for j in range(num_cars + 1):\n",
    "            s = i * (num_cars + 1) + j\n",
    "            actions = np.arange(-max_move, max_move + 1)\n",
    "            actions = np.reshape(actions, (actions.shape[0],1))\n",
    "            all_return = np.zeros(actions.shape)\n",
    "            for k in range(actions.shape[0]):\n",
    "                if ((actions[k] >= 0 and i >= actions[k]) or (actions[k] < 0 and j >= np.absolute(actions[k]))):\n",
    "                    all_return[k] = expected_return((i,j), actions[k], V_s)\n",
    "                else:\n",
    "                    all_return[k] = -np.inf\n",
    "            pi_action[i,j] = actions[np.argmax(all_return)]\n",
    "    \n",
    "    print('Updated policy using new value function\\n', pi_action.reshape(num_cars + 1, num_cars + 1))\n",
    "    if np.all(pi_action == old_policy):\n",
    "        print('\\n\\n***Found optimal policy***\\n\\n')\n",
    "        break\n",
    "    print('\\n\\n ***Iteration Complete, still searching for optimal policy***\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "fig = plt.figure()\n",
    "ax = sns.heatmap(pi_action.reshape(num_cars+1, num_cars+1))\n",
    "fig.savefig('original jack problem.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cars = 10\n",
    "num_states = (num_cars + 1) * (num_cars + 1)\n",
    "\n",
    "num_rentals_1 = 3\n",
    "num_returns_1 = 3\n",
    "\n",
    "num_rentals_2 = 4\n",
    "num_returns_2 = 2\n",
    "\n",
    "parking_space_limit = 5\n",
    "\n",
    "r_rent = 10\n",
    "r_move = -2\n",
    "r_cars_more_than_5 = -4\n",
    "\n",
    "max_move = 5\n",
    "\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_return_modified(state, action, value_function):\n",
    "    e_return = 0.0\n",
    "    if action <=0:\n",
    "        e_return += r_move * np.absolute(action)\n",
    "    new_state = min(state[0] - action, num_cars), min(state[1] + action, num_cars)\n",
    "    new_state = int(new_state[0]), int(new_state[1])\n",
    "    if new_state[0] > parking_space_limit:\n",
    "        e_return += r_cars_more_than_5\n",
    "    if new_state[1] > parking_space_limit:\n",
    "        e_return += r_cars_more_than_5\n",
    "    for i in range(0, new_state[0] + 1):\n",
    "        for j in range(0, new_state[1] + 1):\n",
    "            p_rent = poisson(i, num_rentals_1) * poisson(j, num_rentals_2)\n",
    "            rent_reward = (i + j) * r_rent\n",
    "            for k in range(0, 11):\n",
    "                for l in range(0, 11):\n",
    "                    p_return = poisson(k, num_returns_1) * poisson(l, num_returns_2)\n",
    "                    final_state = new_state[0] - i + k, new_state[1] - j + l\n",
    "                    final_state = min(final_state[0], num_cars), min(final_state[1], num_cars)\n",
    "                    n_s = final_state[0] * (num_cars + 1) + final_state[1]\n",
    "                    e_return += p_return * p_rent * (rent_reward + gamma * value_function[int(n_s)])\n",
    "    \n",
    "    return e_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update value function for policy\n",
      " [[0.00000000e+00 3.64643126e-02 1.82325596e-01 4.74083652e-01\n",
      "  8.63191911e-01 1.25245145e+00 1.56404443e+00 1.77197599e+00\n",
      "  1.89099401e+00 1.95067999e+00 1.97734863e+00]\n",
      " [2.73843741e-02 2.82814957e-01 1.08579509e+00 2.54781630e+00\n",
      "  4.40497724e+00 6.21135405e+00 7.63448754e+00 8.57839708e+00\n",
      "  9.12122426e+00 9.39882705e+00 9.52825420e+00]\n",
      " [1.10516959e-01 8.63466228e-01 2.99486518e+00 6.68766832e+00\n",
      "  1.12629753e+01 1.56674100e+01 1.91436460e+01 2.14870081e+01\n",
      "  2.28850654e+01 2.36494510e+01 2.40465709e+01]\n",
      " [2.39953347e-01 1.67960152e+00 5.53392157e+00 1.20324552e+01\n",
      "  1.99977826e+01 2.76766833e+01 3.38235499e+01 3.80975988e+01\n",
      "  4.07895559e+01 4.23890469e+01 4.33185029e+01]\n",
      " [3.78185205e-01 2.50493749e+00 8.02201136e+00 1.71860243e+01\n",
      "  2.83779250e+01 3.92353369e+01 4.80810633e+01 5.44342276e+01\n",
      "  5.86431806e+01 6.13216229e+01 6.30075739e+01]\n",
      " [4.91505527e-01 3.16042420e+00 9.96289938e+00 2.11752453e+01\n",
      "  3.48677926e+01 4.82504632e+01 5.93346003e+01 6.75190445e+01\n",
      "  7.31624698e+01 7.69371182e+01 7.94421776e+01]\n",
      " [5.68032347e-01 3.59492202e+00 1.12382474e+01 2.37949790e+01\n",
      "  3.91570887e+01 5.42819970e+01 6.69873992e+01 7.65806181e+01\n",
      "  8.34003684e+01 8.81288685e+01 9.13830769e+01]\n",
      " [6.13376586e-01 3.84987655e+00 1.19856944e+01 2.53403271e+01\n",
      "  4.17210263e+01 5.79549976e+01 7.17506705e+01 8.23506411e+01\n",
      "  9.00620005e+01 9.55516931e+01 9.94295300e+01]\n",
      " [6.38378302e-01 3.99006305e+00 1.23988683e+01 2.62056230e+01\n",
      "  4.31846057e+01 6.01022153e+01 7.46085271e+01 8.59028371e+01\n",
      "  9.42615674e+01 1.00328653e+02 1.04696825e+02]\n",
      " [6.51848608e-01 4.06578589e+00 1.26242758e+01 2.66856474e+01\n",
      "  4.40146848e+01 6.13513785e+01 7.63155843e+01 8.80791679e+01\n",
      "  9.68944171e+01 1.03383751e+02 1.08121572e+02]\n",
      " [6.59167371e-01 4.10960949e+00 1.28301415e+01 2.75679294e+01\n",
      "  4.66606724e+01 6.70896057e+01 8.62875720e+01 1.03045655e+02\n",
      "  1.17203792e+02 1.29018117e+02 1.38785506e+02]]\n",
      "Updated policy using new value function\n",
      " [[ 0.  0.  0.  0. -1. -2. -2. -3. -3. -4. -4.]\n",
      " [ 0.  0.  0.  0. -1. -1. -2. -2. -3. -3. -4.]\n",
      " [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -2.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1. -1. -2.]\n",
      " [ 2.  2.  2.  1.  1.  1.  0.  0.  0. -1. -1.]\n",
      " [ 3.  3.  2.  2.  2.  1.  1.  0.  0.  0. -1.]\n",
      " [ 4.  3.  3.  3.  2.  2.  1.  1.  0.  0.  0.]\n",
      " [ 4.  4.  4.  3.  3.  2.  2.  1.  1.  0.  0.]\n",
      " [ 5.  5.  4.  4.  3.  3.  2.  2.  1.  1.  0.]\n",
      " [ 5.  5.  5.  4.  4.  3.  3.  2.  2.  1.  0.]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal policy***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[0.00000000e+00 3.64643126e-02 1.82325596e-01 4.74083652e-01\n",
      "  5.43837040e-01 2.61932550e+00 7.09476568e+00 1.32064230e+01\n",
      "  2.02560204e+01 2.77555977e+01 3.48874498e+01]\n",
      " [2.74025547e-02 2.82948444e-01 1.08635747e+00 2.54961387e+00\n",
      "  4.63718286e+00 9.14081691e+00 1.52809210e+01 2.24048520e+01\n",
      "  2.99035175e+01 3.71349937e+01 4.26971180e+01]\n",
      " [1.10740498e-01 8.65026091e-01 3.00103968e+00 6.70611350e+00\n",
      "  1.13089195e+01 1.75544046e+01 2.49134452e+01 3.24692867e+01\n",
      "  4.00170198e+01 4.53988826e+01 5.10511810e+01]\n",
      " [2.40502196e-01 1.68343057e+00 5.54909951e+00 1.20779579e+01\n",
      "  2.01116987e+01 2.79242430e+01 3.57090824e+01 4.38956638e+01\n",
      "  4.91427682e+01 5.53968108e+01 5.93462671e+01]\n",
      " [3.78838787e-01 3.81643025e+00 1.06420075e+01 1.91139632e+01\n",
      "  2.86089583e+01 3.96330622e+01 4.87516662e+01 5.41984625e+01\n",
      "  6.14209455e+01 6.63066374e+01 6.68552968e+01]\n",
      " [2.25494591e+00 9.55771114e+00 1.86432250e+01 2.83901096e+01\n",
      "  4.03352320e+01 5.04568418e+01 6.03868937e+01 6.88183090e+01\n",
      "  7.48278838e+01 7.53895084e+01 7.91152409e+01]\n",
      " [8.67455798e+00 1.84680313e+01 2.87711987e+01 4.17098884e+01\n",
      "  5.28344729e+01 6.35215086e+01 7.31266375e+01 7.87614085e+01\n",
      "  8.55152308e+01 9.02467468e+01 8.58482235e+01]\n",
      " [1.81720314e+01 2.94070249e+01 4.32857090e+01 5.52676840e+01\n",
      "  6.71485561e+01 7.77648770e+01 8.45582914e+01 9.23513777e+01\n",
      "  9.36193220e+01 9.84602292e+01 1.01750873e+02]\n",
      " [2.97359384e+01 4.43918527e+01 5.70175614e+01 7.06091246e+01\n",
      "  8.19985653e+01 9.04514504e+01 9.90760780e+01 1.01950909e+02\n",
      "  1.07604753e+02 1.05297882e+02 1.08460501e+02]\n",
      " [4.45703777e+01 5.76215671e+01 7.30926332e+01 8.49984971e+01\n",
      "  9.56567418e+01 1.04847918e+02 1.09836835e+02 1.16044780e+02\n",
      "  1.15749129e+02 1.19416214e+02 1.14358226e+02]\n",
      " [5.83442124e+01 7.41388194e+01 8.63770462e+01 9.95804237e+01\n",
      "  1.09223391e+02 1.18096325e+02 1.24884601e+02 1.31418143e+02\n",
      "  1.35930446e+02 1.43914799e+02 1.55875863e+02]]\n",
      "Updated policy using new value function\n",
      " [[ 0.  0.  0.  0. -1. -2. -2. -3. -3. -4. -4.]\n",
      " [ 0.  0.  0.  0. -1. -1. -2. -2. -3. -3. -4.]\n",
      " [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -3.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1. -2. -3.]\n",
      " [ 2.  2.  2.  1.  1.  0.  0.  0. -1. -2. -2.]\n",
      " [ 3.  3.  2.  2.  1.  1.  0.  0. -1. -1. -2.]\n",
      " [ 4.  3.  3.  2.  2.  1.  1.  0.  0. -1. -1.]\n",
      " [ 4.  4.  3.  3.  2.  2.  1.  0.  0.  0. -1.]\n",
      " [ 5.  4.  4.  3.  3.  2.  1.  1.  0.  0.  0.]\n",
      " [ 5.  5.  4.  4.  3.  2.  2.  1.  0.  0.  0.]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal policy***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[0.00000000e+00 3.64643126e-02 1.82325596e-01 4.74083652e-01\n",
      "  5.43837040e-01 2.61932550e+00 7.09476568e+00 1.32064230e+01\n",
      "  2.02560204e+01 2.77555977e+01 3.48874498e+01]\n",
      " [2.74025547e-02 2.82948444e-01 1.08635747e+00 2.54961387e+00\n",
      "  4.63718286e+00 9.14081691e+00 1.52809210e+01 2.24048520e+01\n",
      "  2.99035175e+01 3.71349937e+01 4.26971180e+01]\n",
      " [1.10740498e-01 8.65026091e-01 3.00103968e+00 6.70611350e+00\n",
      "  1.13089195e+01 1.75544046e+01 2.49134452e+01 3.24692867e+01\n",
      "  4.00170198e+01 4.53988826e+01 5.10511810e+01]\n",
      " [2.40502196e-01 1.68343057e+00 5.54909951e+00 1.20779579e+01\n",
      "  2.01116987e+01 2.79242430e+01 3.57090824e+01 4.38956638e+01\n",
      "  4.91427682e+01 5.53968108e+01 5.68814129e+01]\n",
      " [3.78838587e-01 3.81641849e+00 1.06419508e+01 1.91137272e+01\n",
      "  2.86087110e+01 3.96321645e+01 4.87487697e+01 5.41960532e+01\n",
      "  6.14142098e+01 6.23111037e+01 6.03582419e+01]\n",
      " [2.25482364e+00 9.55718793e+00 1.86412663e+01 2.83877808e+01\n",
      "  4.03275681e+01 4.90893847e+01 6.03346631e+01 6.87308796e+01\n",
      "  6.96872552e+01 6.68214630e+01 7.16017894e+01]\n",
      " [8.66710558e+00 1.84500105e+01 2.87464368e+01 4.16557985e+01\n",
      "  5.08991601e+01 6.33573567e+01 6.89873286e+01 7.84118324e+01\n",
      "  7.55030436e+01 8.12571472e+01 7.49639818e+01]\n",
      " [1.81211578e+01 2.93292357e+01 4.31355061e+01 5.32421609e+01\n",
      "  6.67837848e+01 7.32779830e+01 8.38431721e+01 8.57398613e+01\n",
      "  9.25480039e+01 8.60880959e+01 8.96661510e+01]\n",
      " [2.95996239e+01 4.41487620e+01 5.56246397e+01 7.00519951e+01\n",
      "  7.78693807e+01 8.94029679e+01 9.26308400e+01 9.19485784e+01\n",
      "  9.87718439e+01 1.03105714e+02 9.38207371e+01]\n",
      " [4.42476447e+01 5.72810568e+01 7.23310232e+01 8.19981793e+01\n",
      "  9.42069830e+01 9.92192194e+01 1.00095996e+02 1.07782354e+02\n",
      "  1.04431376e+02 1.08517070e+02 1.10909093e+02]\n",
      " [5.77596065e+01 7.32233881e+01 8.50233520e+01 9.77792561e+01\n",
      "  1.06093239e+02 1.12345622e+02 1.21522879e+02 1.29239199e+02\n",
      "  1.40440763e+02 1.47793953e+02 1.53246973e+02]]\n",
      "Updated policy using new value function\n",
      " [[ 0.  0.  0.  0. -1. -2. -2. -3. -3. -4. -4.]\n",
      " [ 0.  0.  0.  0. -1. -1. -2. -2. -3. -3. -4.]\n",
      " [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -3.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1. -2. -3.]\n",
      " [ 2.  2.  2.  1.  1.  0.  0.  0. -1. -2. -2.]\n",
      " [ 3.  3.  2.  2.  1.  1.  0.  0. -1. -1. -2.]\n",
      " [ 4.  3.  3.  2.  2.  1.  1.  0.  0. -1. -2.]\n",
      " [ 4.  4.  3.  3.  2.  2.  1.  0.  0. -1. -1.]\n",
      " [ 5.  4.  4.  3.  3.  2.  1.  0.  0.  0. -1.]\n",
      " [ 5.  5.  4.  4.  3.  2.  1.  1.  0.  0.  0.]]\n",
      "\n",
      "\n",
      " ***Iteration Complete, still searching for optimal policy***\n",
      "\n",
      "\n",
      "Update value function for policy\n",
      " [[0.00000000e+00 3.64643126e-02 1.82325596e-01 4.74083652e-01\n",
      "  5.43837040e-01 2.61932550e+00 7.09476568e+00 1.32064230e+01\n",
      "  2.02560204e+01 2.77555977e+01 3.48874498e+01]\n",
      " [2.74025547e-02 2.82948444e-01 1.08635747e+00 2.54961387e+00\n",
      "  4.63718286e+00 9.14081691e+00 1.52809210e+01 2.24048520e+01\n",
      "  2.99035175e+01 3.71349937e+01 4.26971180e+01]\n",
      " [1.10740498e-01 8.65026091e-01 3.00103968e+00 6.70611350e+00\n",
      "  1.13089195e+01 1.75544046e+01 2.49134452e+01 3.24692867e+01\n",
      "  4.00170198e+01 4.53988826e+01 5.10511810e+01]\n",
      " [2.40502196e-01 1.68343057e+00 5.54909951e+00 1.20779579e+01\n",
      "  2.01116987e+01 2.79242430e+01 3.57090824e+01 4.38956638e+01\n",
      "  4.91427682e+01 5.53968108e+01 5.68814129e+01]\n",
      " [3.78838587e-01 3.81641849e+00 1.06419508e+01 1.91137272e+01\n",
      "  2.86087110e+01 3.96321645e+01 4.87487697e+01 5.41960532e+01\n",
      "  6.14142098e+01 6.23111037e+01 6.03582419e+01]\n",
      " [2.25482364e+00 9.55718793e+00 1.86412663e+01 2.83877808e+01\n",
      "  4.03275681e+01 4.90893847e+01 6.03346631e+01 6.87308796e+01\n",
      "  6.96872552e+01 6.68214630e+01 7.16017894e+01]\n",
      " [8.66710558e+00 1.84500105e+01 2.87464368e+01 4.16557985e+01\n",
      "  5.08991601e+01 6.33573567e+01 6.89873286e+01 7.84118324e+01\n",
      "  7.55030436e+01 8.12571472e+01 7.49639818e+01]\n",
      " [1.81211578e+01 2.93292357e+01 4.31355061e+01 5.32421609e+01\n",
      "  6.67837848e+01 7.32779830e+01 8.38431721e+01 8.57398613e+01\n",
      "  9.25480039e+01 8.60880959e+01 7.77902957e+01]\n",
      " [2.95989565e+01 4.41463146e+01 5.56207961e+01 7.00395720e+01\n",
      "  7.78528576e+01 8.93564204e+01 9.25786325e+01 9.18994134e+01\n",
      "  9.86544263e+01 9.01259331e+01 9.34149417e+01]\n",
      " [4.42411426e+01 5.72698293e+01 7.23004954e+01 8.19534956e+01\n",
      "  9.40994053e+01 9.90862632e+01 9.99560174e+01 9.73984653e+01\n",
      "  1.03944043e+02 1.07826636e+02 9.69902416e+01]\n",
      " [5.77283319e+01 7.31539764e+01 8.49081043e+01 9.75507838e+01\n",
      "  1.05770692e+02 1.11946715e+02 1.19220501e+02 1.28402568e+02\n",
      "  1.39563218e+02 1.46589967e+02 1.51651842e+02]]\n"
     ]
    }
   ],
   "source": [
    "V_s = np.zeros((num_states,1)) # Value functions for all states\n",
    "pi_action = np.zeros((num_cars + 1, num_cars + 1)) # Initial policy to not move any cars\n",
    "tolerance = 1.0e-1\n",
    "\n",
    "policy_stable = False\n",
    "while not policy_stable:\n",
    "    # Policy evaluation\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        v_s = np.copy(V_s)\n",
    "        V_s = np.zeros(V_s.shape)\n",
    "        for i in range(num_cars + 1):\n",
    "            for j in range(num_cars + 1):\n",
    "                s = i * (num_cars + 1) + j\n",
    "                V_s[s] = expected_return((i,j), pi_action[i,j], V_s)\n",
    "                delta = max(delta, V_s[s] - v_s[s])\n",
    "        if delta < tolerance:\n",
    "            break\n",
    "            \n",
    "    print('Update value function for policy\\n', V_s.reshape((num_cars + 1,num_cars + 1)))\n",
    "    \n",
    "    # Policy improvement\n",
    "    old_policy = np.copy(pi_action)\n",
    "    pi_action = np.zeros(pi_action.shape)\n",
    "    for i in range(num_cars + 1):\n",
    "        for j in range(num_cars + 1):\n",
    "            s = i * (num_cars + 1) + j\n",
    "            actions = np.arange(-max_move, max_move + 1)\n",
    "            actions = np.reshape(actions, (actions.shape[0],1))\n",
    "            all_return = np.zeros(actions.shape)\n",
    "            for k in range(actions.shape[0]):\n",
    "                if ((actions[k] >= 0 and i >= actions[k]) or (actions[k] < 0 and j >= np.absolute(actions[k]))):\n",
    "                    all_return[k] = expected_return((i,j), actions[k], V_s)\n",
    "                else:\n",
    "                    all_return[k] = -np.inf\n",
    "            pi_action[i,j] = actions[np.argmax(all_return)]\n",
    "    \n",
    "    print('Updated policy using new value function\\n', pi_action.reshape(num_cars + 1, num_cars + 1))\n",
    "    if np.all(pi_action == old_policy):\n",
    "        print('\\n\\n***Found optimal policy***\\n\\n')\n",
    "        break\n",
    "    print('\\n\\n ***Iteration Complete, still searching for optimal policy***\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig = plt.figure()\n",
    "ax = sns.heatmap(pi_action.reshape(num_cars+1, num_cars+1))\n",
    "fig.savefig('4.7.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
